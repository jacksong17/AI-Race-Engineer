# Demo Scenarios: Interactive Driver Feedback

**Three powerful scenarios to demonstrate agentic flexibility**

---

## Scenario 1: Data Confirms Driver (Loose Car)

### Command:
```bash
python demo.py "Car feels loose off corners, rear end wants to come around"
```

### What Happens:
```
Agent 1: üí° DIAGNOSIS: Oversteer (loose rear end)
         ‚úì DECISION: Prioritize REAR GRIP parameters
         Priority: tire_psi_rr, tire_psi_lr, track_bar_height_left

Agent 2: üéØ Focusing on: tire_psi_rr, tire_psi_lr
         Results: tire_psi_rr +0.551 [PRIORITY - matches driver feedback]

Agent 3: ‚úÖ VALIDATION: Top parameter matches driver feedback!
         Data confirms: tire_psi_rr is primary factor
         STRONG RECOMMENDATION: Reduce tire_psi_rr
         üéß Addresses: Oversteer (loose rear end)
```

### Key Talking Point:
*"The driver said 'loose rear' and the data confirmed it - tire_psi_rr correlation of 0.551. This shows agents combining qualitative driver expertise with quantitative data analysis."*

---

## Scenario 2: Data Contradicts Driver (Tight Car)

### Command:
```bash
python demo.py "Front end pushes in turn 1 and 2"
```

### What Happens:
```
Agent 1: üí° DIAGNOSIS: Understeer (tight front end)
         ‚úì DECISION: Prioritize FRONT GRIP parameters
         Priority: tire_psi_lf, tire_psi_rf, cross_weight

Agent 2: üéØ Focusing on: tire_psi_lf, cross_weight, spring_lf
         Results: tire_psi_rr +0.551 (not in priority list)
                  cross_weight -0.289 [PRIORITY - matches driver feedback]

Agent 3: ‚ö†Ô∏è  INSIGHT: Data suggests different root cause than driver feedback
         Driver complaint: Understeer (tight front end)
         Data indicates: tire_psi_rr (not in priority list)
         STRONG RECOMMENDATION: Reduce tire_psi_rr
```

### Key Talking Point:
*"Driver said 'tight front end' but the data shows rear tire pressure is the real issue. The agents reasoned independently and provided a data-driven recommendation that might contradict driver feel. This is like an experienced engineer saying 'I hear you, but let's check the data first.'"*

---

## Scenario 3: Complex/Vague Feedback (General Optimization)

### Command:
```bash
python demo.py "Car feels okay but I think we can find more speed"
```

### What Happens:
```
Agent 1: üí° DIAGNOSIS: General optimization needed
         ‚úì DECISION: Broad analysis of all parameters

Agent 2: üîç Evaluating all 8 features (no priority filtering)
         Results: tire_psi_rr +0.551 (strongest correlation)

Agent 3: ‚úì DECISION: Strong signal detected
         RECOMMENDATION: Reduce tire_psi_rr
```

### Key Talking Point:
*"When driver feedback is vague, Agent 1 doesn't guess - it analyzes ALL parameters and lets the data speak. Different input ‚Üí different reasoning strategy."*

---

## Scenario 4: Suspension Issues (Bottoming)

### Command:
```bash
python demo.py "Suspension is bottoming out in the corners"
```

### What Happens:
```
Agent 1: üí° DIAGNOSIS: Suspension bottoming - insufficient spring stiffness
         ‚úì DECISION: Prioritize SPRING RATES
         Priority: spring_lf, spring_rf, spring_lr, spring_rr

Agent 2: üéØ Focusing on: spring_lf, spring_rf
         Results: spring_lf +0.251 [PRIORITY - matches driver feedback]
                  spring_rf +0.254 [PRIORITY - matches driver feedback]

Agent 3: ‚úì DECISION: Moderate signal
         RECOMMENDATION: Consider spring adjustments
```

### Key Talking Point:
*"Agent 1 recognized 'bottoming' relates to spring stiffness, not tire pressure. Agents have domain knowledge about what parameters affect which handling characteristics."*

---

## üéØ The "Wow" Demo Flow (5 minutes)

### 1. Setup (30 seconds)
"Let me show you the system's flexibility. I'll give it completely different driver complaints and watch how the agents adapt."

### 2. First Run - Loose Car (2 minutes)
```bash
python demo.py "Car feels loose off corners"
```

**Point out:**
- Agent 1 prioritizes rear parameters
- Agent 2 marks rear tire pressures as [PRIORITY]
- Agent 3 validates data matches driver complaint
- **Recommendation:** Reduce rear tire pressure

### 3. Second Run - Tight Car (2 minutes)
```bash
python demo.py "Front end pushes"
```

**Point out:**
- Agent 1 prioritizes front parameters
- Agent 2 marks front parameters as [PRIORITY]
- Agent 3 finds data contradicts driver (INSIGHT!)
- **Recommendation:** Different from first run!

### 4. Key Insight (30 seconds)
"Same code, same Bristol data, but completely different analysis paths. The agents:
1. **Perceived** different driver feedback
2. **Reasoned** about different root causes
3. **Planned** different analysis strategies
4. **Acted** with different recommendations

That's true agentic behavior - context-aware decision making, not sequential processing!"

---

## üîë Why This Demonstrates TRUE Agents

### ‚ùå What a Pipeline Would Do:
- Always analyze the same parameters
- Always produce the same output
- Ignore driver input

### ‚úÖ What Agentic Systems Do:
- **Adapt** analysis based on context
- **Reason** about relationships (symptoms ‚Üí causes)
- **Plan** different strategies for different scenarios
- **Validate** hypotheses against data
- **Provide insights** when data conflicts with expectations

---

## üé¨ Live Demo Tips

### Before Demo:
```bash
# Test both scenarios work
python demo.py "loose off corners"
python demo.py "tight on entry"
```

### During Demo:
- **Type slowly** so audience can read your input
- **Pause** at Agent 1's diagnosis to highlight reasoning
- **Point to [PRIORITY] markers** in Agent 2's output
- **Emphasize** Agent 3's validation or insight

### Backup Plan:
If typing fails or API is slow, use command-line:
```bash
python demo.py "your feedback here"
```

---

## üìä Expected Results Comparison

| Scenario | Agent 1 Priority | Top Data Finding | Agent 3 Validation |
|----------|------------------|------------------|-------------------|
| **Loose Car** | Rear grip (tire_psi_rr, tire_psi_lr) | tire_psi_rr +0.551 | ‚úÖ Matches driver complaint |
| **Tight Car** | Front grip (tire_psi_lf, cross_weight) | tire_psi_rr +0.551 | ‚ö†Ô∏è Data contradicts driver |
| **General** | All parameters | tire_psi_rr +0.551 | ‚ÑπÔ∏è Data-driven only |
| **Bottoming** | Spring rates (spring_lf, spring_rf) | spring_rf +0.254 | ‚úÖ Matches driver complaint |

**Same Bristol data, different recommendations based on driver input!**

---

## üí° Q&A Responses

**Q: "Is the driver feedback just changing what parameters you look at?"**
A: "No - it changes Agent 1's *reasoning* about root causes, which guides Agent 2's *planning*, which affects Agent 3's *validation*. In scenario 2, Agent 3 even detected that data contradicted the driver and provided that insight. A pipeline would just filter parameters; agents reason, plan, and validate."

**Q: "What if the driver gives nonsense feedback?"**
A: "Great question - Agent 1 would classify it as 'general' optimization and Agent 2 would analyze all parameters. The system gracefully degrades to data-driven mode. That's another agentic behavior - handling uncertainty."

**Q: "Could you add more driver feedback options?"**
A: "Absolutely! I could add temperature complaints, brake feel, steering response, etc. The LLM interpreter and Agent 1's reasoning would handle it automatically. That's the beauty of the agentic architecture - it's extensible without rewriting the pipeline."

---

## üöÄ Next Level (If Time Permits)

Show the audience you can add NEW complaints on the fly:

```bash
# Something not explicitly programmed:
python demo.py "Car chatters over the bumps in turn 3"

# Agent 1 will still interpret and prioritize parameters!
```

This proves the system truly reasons about novel inputs, not just pattern matching.
